# 第4章 决策树

## 4.1 基本流程

决策树的基本流程遵循简单且直观的“分而治之”策略。

> **输入**：训练集 $D=\{(\boldsymbol{x_1},y_1),(\boldsymbol{x_2},y_2),\dots,(\boldsymbol{x_m},y_m)\}$；
>
> ​            属性集$A=\{a_1,a_2,\dots,a_d\}$
>
> **过程**：函数$TreeGenerate(D,A)$
>
> 01: 生成结点 node
>
> 02: **if** $D$中样本全属于一个类别$C$ **then**
>
> 03:      将node标记为$C$类叶节点；**return**
>
> 04: **end if**
>
> 05: **if** $A=\O$ **OR** $D$中样本在$A$上取值相同 **then**
>
> 06:      将node标记为叶节点，其类别标记为$D$中样本数最多的类；**return**
>
> 07:  **end if**
>
> 08: 从$A$中选择最优化分属性$a_*$；
>
> 09: **for** $a_*$的每一个值$a_*^v$ **do**
>
> 10:     为node生成一个分支；令$D_v$表示$D$ 中在 $a_*$上取值为$a_*^v$的样本子集；
>
> 11:     **if** $D_v$ 为空 **then**
>
> 12:         将分支结点标记为叶结点，其类别标记为$D$中样本最多的类；**return**
>
> 13:     **else**
>
> 14:         以$TreeGenerate(D_v,A \setminus \{a_*\})$为分支结点
>
> 15:      **end if**
>
> 16: **end for**
>
> **输出：**以node为根结点的一棵决策树



## 4.2 划分选择


### 4.2.1 信息增益

信息熵是度量样本集合纯度最常用的一种指标。

假定样本集合$D$中第$k$类样本所占比例为$p_k$（$k=1,2,\dots,|\mathcal{y}|$），则$D$的信息熵为：
$$
Ent(D)=-\sum_{k=1}^{|\mathcal{y}|} p_k\log_2p_k
$$
$Ent(D)$的值越小，则$D$的纯度越高。

通过4.1中划分样本，所得到的“信息增益”为
$$
Gain(D,a)=Ent(D)-\sum_{v=1}^V \frac{D^v}{D}Ent(D^v)
$$


### 4.2.2 增益率

信息增益准则对可取值数目较多的属性有所偏好，为减少这种偏好可能带来的不利影响，可以使用“增益率”：
$$
Gain_ratio(D,a)=\frac{Gain(D,a)}{IV(a)}
$$
其中
$$
IV(a)=-\sum_{v=1}^{V}\frac{|D^v|}{|D|}\log_2\frac{|D^v|}{|D|}
$$
称为属性$a$的“固有值”，属性$a$的可能取值数目越多，则$IV(a)$越大。

增益率准则对可取值数目较少的属性有所偏好。



### 4.2.3 基尼指数

CART决策树使用“基尼指数”来选择划分属性，数据集$D$的纯度可用的基尼值来度量：
$$
Gini(D)=\sum_{k=1}^{|\mathcal{y}|}\sum_{k'\neq k}p_kp_{k'}=1-\sum_{k=1}^{|\mathcal{y}|}p_k^2
$$
直观来说，$Gini(D)$反映了从数据集$D$中随机抽取两个样本，其类别标记不一致的概率，因此$Gini(D)$越小，则数据集$D$的纯度越高。

属性$a$的基尼指数定义为
$$
Gini\_index(D,a)=\sum_{v=1}^{V}\frac{|D^v|}{|D|}Gini(D^v)
$$


## 4.3 剪枝处理

剪枝是决策树学习算法对付”过拟合“的主要手段。

剪枝策略基本策略有”预剪枝“和”后剪枝“。

预剪枝是指在决策树生成过程中，对每个结点在划分前进行评估，若当前结点的划分不能带来决策树泛化性能提升，则停止划分，将当前结点标记为叶结点。

后剪枝是先从训练集生成一棵完整的决策树，然后从低向上地对非叶结点进行考察，若将该结点对应的子树替代为叶结点能带来决策树泛化性能提升，则将子树替换为叶结点。



## 4.4 连续与缺失值

### 4.4.1 连续值处理

连续属性离散化技术

### 4.4.2 缺失值处理

需解决两个问题：

1. 如何在属性值缺失的情况下进行划分属性选择？

2. 给定划分属性，若样本在该属性上值的缺失，如何对样本进行划分？






