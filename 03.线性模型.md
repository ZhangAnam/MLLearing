# 第3章 线性模型

## 3.1 基本形式

给定由$d$个属性描述的示例$\boldsymbol{x}=(x_1;x_2;\ldots;x_d)$。

线性模型（Linear Model | LM）试图学得一个属性线性组合的预测模型，即
$$
f(\boldsymbol{x})=w_1x_1+w_2x_2+\cdots+w_dx_d+b
$$
向量形式
$$
f(\boldsymbol{x})=\boldsymbol{w}^\rm{T}\boldsymbol{x}+b
$$

* 线性模型形式简单、易于建模
* 许多功能更为强大的非线性模型，可以在星型模型基础上，通过引入层级结构或高位映射而得
* $\boldsymbol{w}$直观表达各个属性的重要性，LM具有很好的可解释性（comprehensibility）



## 3.2 线性回归 （Linear Regression）

**模型目标**

试图学得$ f(\boldsymbol{x_i})=\boldsymbol{w}^\rm{T}\boldsymbol{x_i}+b $，使得$ f(\boldsymbol{x_i})\simeq y_i $

**最小二乘法**

令
$$
\hat{y_i}=f(\vec{x_i})=\boldsymbol{w}^T \vec{x_i}+b
$$

$$
\boldsymbol{\hat{w}}=(w_1,w_2,\dots,w_d,b)^T
$$

$$
\boldsymbol{\tilde{x_i}}=(x_{i1},x_{i2},\dots,x_{id},1)^T
$$

$$
\boldsymbol{y}=(y_1,y_2,\dots,y_m)
$$

$$
X=\begin{pmatrix}x_{11} &x_{12} &\dots & x_{1d} & 1 \\ x_{21} &x_{22} &\dots &x_{2d} & 1 \\ \vdots &\vdots & \ddots &\vdots &\vdots \\ x_{m1} & x_{m2} &\dots &x_{md} & 1 \end{pmatrix}= \begin{pmatrix}\boldsymbol{x_1^T} &1 \\ \boldsymbol{x_2^T} &1 \\  \vdots & \vdots \\\boldsymbol{x_m^T} &1 \\  \end{pmatrix}
$$

定义损失函数 
$$
\begin{equation}
\begin{split}
L(f) &=\sum_{i=1}^N (\hat{y_i}-y_i)^2 \\
&=\sum_{i=1}^N (\boldsymbol{w}^T \vec{x_i}+b-y_i)^2 \\
&=(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\hat{w}})^T(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\hat{w}})
\end{split}
\end{equation}
$$
则目标
$$
\boldsymbol{\hat{w}}^*=\underset{\boldsymbol{\hat{w}}}{\arg\min} L(f)
$$
对$\hat{\boldsymbol{w}}  $求导
$$
\begin{equation}
\begin{split}
\def\Q#1#2{\frac{\partial#1}{\partial #2}}
\def\P{\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\hat{w}}}
\Q{E_{\boldsymbol{\hat{w}}}}{\boldsymbol{\hat{w}}}
&=\Q{(\P)^T(\P)}{(\P)} \Q{(\P)}{\boldsymbol{\hat{w}}} \\
&=2(\P)(-\boldsymbol{X}^T) \\
&=2\boldsymbol{X}^T(\boldsymbol{X}\boldsymbol{\hat{w}}-\boldsymbol{y}) \\
&=2(\boldsymbol{X}^T\boldsymbol{X}\boldsymbol{\hat{w}}-\boldsymbol{X}^T\boldsymbol{y})
\end{split}
\end{equation}
$$
导数为0时，可得$\hat{\boldsymbol{w}}  $的最优解的闭式解

$ \boldsymbol{X}^T\boldsymbol{X}$ **满秩时**
$$
{\boldsymbol{\hat{w}}}^*=(\boldsymbol{X}^T\boldsymbol{X}\boldsymbol)^{-1}\boldsymbol{X}^T\boldsymbol{y}
$$
$ \boldsymbol{X}^T\boldsymbol{X}$ **不满秩时**需要引入正则化（regularization），见下文。不满秩时，会输出多个 $\boldsymbol{\hat{w}}^*  $，选择哪一个作为输出，有归纳偏好决定。



**广义线性模型**

1. 对数线性回归
   $$
   \ln y=\boldsymbol{w}^\rm{T}\boldsymbol{x}+b
   $$

2. 广义线性模型

   更一般地，考虑单调可微函数$g(\cdot)$
   $$
   y=g^{-1}(\boldsymbol{w}^\rm{T}\boldsymbol{x}+b)
   $$
   $g(\cdot)$称为“联系函数”




## 3.3 对数几率回归 （逻辑回归 Logistic Regression）



## 3.4 线性判别分析 （Linear Discriminant Analysis, LDA）



## 3.5 多分类学习



## 3.6 类别不平衡问题





Ridge回归、Lasso回归、ElasticNet回归、Lasso求解方法